---
title: "CDEC_QA_4"
author: "Catarina Pien"
date: "September 17, 2018"
output: html_document
description: QAQC Steps for CDEC data
editor_options: 
chunk_output_type: console
---

Start by clearing the environment.

```{r clean, include = FALSE}
rm(list=ls(all=TRUE))
```

## Load files, edit variable names

* Filter out those that are not contiguous/ not active
* Add datetime sorting variables
```{r setup}
library(tidyverse)
library(readr)
library(lubridate)
library(TTR) # rate of change
library(caTools) # rate of change

#setwd("C:/Users/cpien/OneDrive - California Department of Water Resources/Work/ClimateChange/R_code/")
#### Read files ###
temp_H_0 <- readRDS("TempData/Temp_all_H.rds")

# Filter out stations that are not contiguous.
temp_H <- temp_H_0 %>% 
  filter(!station %in% c("CNT", "CPP", "DAR", "DMC", "DYR","ECD", "HBP", "KA0", "ROR", "DV7", "BOY"))

# Remove unnecessaries
rm(temp_H_0)
```

## QA1) Remove days with less than 20 values 

1. Count the number of rows per station, date group. (There should only be one row per date)
2. To be deleted: Filter out dates for which there are less than 20 values (out of 24).
3. Use antijoin to remove the deleted dates from original data frame.

```{r Missing Values, message = FALSE}

# This data frame contains all the dates with less than 20 values. 
temp_q1_a <- temp_H %>%
  group_by(station, date) %>%
  arrange(station, date, hour) %>%
  summarize(total = n()) %>%
  mutate(Flag_QC1 = ifelse(total < 20, "Y", "N"))

# Values that are flagged
temp_q1_b <- temp_q1_a %>%
  filter(Flag_QC1 == "Y")

#Join original dataframe with flagged values based on values NOT in common. 
#based on station and date
temp_q1 <- temp_H %>%
  left_join(temp_q1_a, by = c("station", "date"))

# Remove unnecessaries
#rm(temp_q1_a)
gc()

```

## QA2) Remove if there are more than 12 repeating values in a row

With help from Michael K!

1. Create new columns indicating whether the temperature at x hour is the same as that of x-1 hours.
2. Take a cumulative sum of all the rows where temperatures are different
3. Group by the sum and count up the number of rows where the temperature is the same.
4. Flag the rows where number of repeated values is above our cut-off
5. Filter out rows that we flagged


```{r repeating values, message = FALSE}

# Function to determine whether values are repeating by each station
# Inputs are data frame and x (number of repeating values you want to check for)

repeating_vals = function(df, x){
  df$same = ifelse(df$Temp == lag(df$Temp, 1, default = 0), 1L, 0L)
  df = df %>%
    mutate(issame = cumsum(df$same == 0L)) #%>%
    df = df %>%group_by(station, issame) %>%
    mutate(flag = sum(same)+1 ) %>%
    ungroup() %>%
    mutate(Flag_repeats = ifelse(flag > x, "Y", "N"))
  return(df)
}

#test it real quick
#test = temp_q1[1:200000,]

#test2 = repeating_vals(test, 10)
#bads = filter(test2, Flag_repeats == "Y")
#OMG IT WORKED!


#Techinically, "==" shouldn't work for decimals

### --------------------------------------------------
temp_q1$Temp <- as.numeric(temp_q1$Temp)

# Run function repeating values and get rid of the columns we don't need
temp_q2_a <- repeating_vals(df = temp_q1, x = 10) %>%
  select(-flag, -issame, -same)



###-------------------------------------------------------------


# Create a dataframe of all the flagged rows
temp_q2_b <- temp_q2_a %>%
  filter(Flag_repeats == "Y")

# Remove unnecessaries
#rm(temp_q2_a, temp_q1)
gc()

```

### QA3: Rate of change and Spike test

Unfortnately, this method doesn't deal with groups of bad data (like when the sonde has been out of the water for a few hours). It gets rid of the first and last "bad" dat ponit, but not the ones in between. We may need to group the data based on the bad data points and use a cumsum to identify the groups. Then compare group mean?

```{r Rate of Change}
### ---------------------------------------------
# Q3: Temp - Temp-1
# sdev_th: Determined threshold for too high of a rate of change (3 * SD(Temp) over 25 days)
# If Q3 > sdev_th, flag.
# Additionally, if Q3 > 5 (5 degree change in 15 minutes), flag. 

temp_q3_a <- temp_q2_a %>%
  group_by(station) %>%
  arrange(station, datetime) %>%
  mutate(QC3 = abs(Temp- lag(Temp, n = 1, default = 0)))%>%
  mutate(sdev_th = 3 * runSD(Temp, 25))%>%
  mutate(Flag_QC3 = ifelse((QC3 > sdev_th | QC3 > 5), "Y", "N"))  %>%

  mutate(Flag_QC3 = replace(Flag_QC3, is.na(Flag_QC3), "N")) %>%
  ungroup()



temp_q3_b <- temp_q3_a %>%
  filter(Flag_QC3 == "Y")

temp_q3 <- temp_q3_a %>%
  select(-c(sdev_th, QC3))

temptest = temp_q3_a %>%
  mutate(Flag_spike = cumsum(Flag_QC3 == "Y"))

temptest$Flag_spike = cumsum(temptest$Flag_QC3 == "Y")


rm(temp_q3_a)

```



## QA4): Remove anything outside of reasonable temperature range (0-40)
```{r 0-40, message = FALSE}
temp_q4 <- temp_q3%>%
mutate(Flag_QC4 = ifelse(Temp<0 | Temp>40, "Y", "N"))

############TEMPORARY ONLY##########

temp_q4d <- temp_q2%>%
  filter(Temp>0 & Temp<40)

```


## Calculating temperature differences 

```{r Temp differences}
Tdiffs_all <- temp_q4d %>%
  group_by(station, date)%>%
  arrange(datetime)%>%
  mutate(Tdiff = Temp - lag(Temp, 1)) %>%
  mutate(large = ifelse(Tdiff>5, "Y", "N")) 

Tdiff_distrib <- Tdiffs_all %>%
  group_by(station) %>%
  summarize(mean.tdiff = mean(Tdiff, na.rm = TRUE),
            max.tdiff = max(Tdiff, na.rm = TRUE),
            n.large = sum(large == "Y", na.rm = TRUE),
            n = n()) %>%
  mutate(percent.large = n.large/n * 100)

ggplot(Tdiffs_all) +
  geom_boxplot(aes(x = station, y =Tdiff))

ggplot(Tdiff_distrib, aes(x = station, y = percent.large)) + 
  geom_point()

```


## How much data are being removed?
```{r Data removal}

removed_all <- temp_q4 %>%
  summarize(Init = n(),
            QC1 = sum(Flag_QC1=="Y"),
            QC2 = sum(Flag_QC2_b == "Y"),
            QC3 = sum(Flag_QC3 == "Y", na.rm = TRUE),
            QC4 = sum(Flag_QC4 == "Y"),
            Remaining = sum(Flag_QC1=="N" & Flag_QC2_b == "N" & Flag_QC3 =="N" & Flag_QC4 =="N", na.rm = TRUE),
            Prop = round(Remaining/Init*100,1))


removed_sta <- temp_q4 %>%
  group_by(station) %>%
  summarize(Init = n(),
            QC1 = sum(Flag_QC1=="Y"),
            QC2 = sum(Flag_QC2_b == "Y"),
            QC3 = sum(Flag_QC3 == "Y", na.rm = TRUE),
            QC4 = sum(Flag_QC4 == "Y"),
            Remaining = sum(Flag_QC1=="N" & Flag_QC2_b == "N" & Flag_QC3 =="N" & Flag_QC4 =="N", na.rm = TRUE),
            Prop = round(Remaining/Init*100,1)) %>%
  arrange(Prop)

a <- as.list(removed_sta)
```


## Write files

```{r WriteFile, message = FALSE}
library(readr)

saveRDS(temp_q4, ".rds")
write_csv(temp_q4, "QC/temp_all_qc.csv")

### Individual files to csv

# write each file as a csv
allsta_q5$station <- as.factor(allsta_q5$station) # need to factorize the "stations"
#Get the list of unique MP names
for (name in levels(allsta_q5$station)) {
  #Subset the data station
  tmp=subset(allsta_q5,station==name)
  #Create a new filename for each station. Designate the folder you want the files in.
  fn=paste('station_files/stations_qa/postoutlier/',name, "_q5.csv", sep="")
  #Save the CSV file for each station
  write_csv(tmp,fn)
}

```


