---
title: "CDEC_QA"
author: "Catarina Pien"
date: "September 17, 2018"
output: html_document
description: QAQC Steps for CDEC data
editor_options: 
  chunk_output_type: console
---

Start by clearing the environment.

```{r clean, include = FALSE}
rm(list=ls(all=TRUE))
```

## Load files, edit variable names

* Filter out those that are not contiguous/ not active
* Add datetime sorting variables
```{r setup}
library(readr)
library(lubridate)
library(dplyr)

setwd("P:/ClimateChange/R_code/")
#### Read files ####
delta_std_0 <- read_csv("station_files/stations_sin/merged/delta_std_sin.csv")
sacR_std_0 <- read_csv("station_files/stations_sin/merged/sacR_std_sin.csv")
sanJR_std_0 <- read_csv("station_files/stations_sin/merged/sanJR_std_sin.csv")

# checking station
check <- delta_std_0 %>% filter(station == "DAR")

# Filter out stations that are not contiguous.
delta_std <- delta_std_0 %>% 
  filter(!station %in% c("CNT", "CPP", "DMC", "DYR","ECD", "HBP", "KA0", "ROR", "DV7", "BOY"))

sacR_std <- sacR_std_0

sanJR_std <- sanJR_std_0 %>%
  filter(!station %in% c("DAR", "TPS"))

# Add some columns to sort by date
delta_std$date <- as.Date(delta_std$datetime, format = "%Y-%m-%d")
delta_std$year <- year(delta_std$datetime)
delta_std$yrMon <- format(as.Date(delta_std$date), "%y-%m")
#delta_std$ymon1 <- as.Date(paste(delta_std$yrMon, "-01", sep=""))
delta_std$hour <- hour(delta_std$datetime)
delta_std$minute <- minute(delta_std$datetime)


sacR_std$date <- as.Date(sacR_std$datetime, format = "%m/%d/%y")
sacR_std$year <- year(sacR_std$datetime)
sacR_std$yrMon <- format(as.Date(sacR_std$date), "%y-%m")
#sacR_std$ymon1 <- as.Date(paste(sacR_std$yrMon, "-01", sep=""))
sacR_std$hour <- hour(sacR_std$datetime)
sacR_std$minute <- minute(sacR_std$datetime)

sanJR_std$date <- as.Date(sanJR_std$datetime, format = "%Y-%m-%d")
sanJR_std$year <- year(sanJR_std$datetime)
sanJR_std$yrMon <- format(as.Date(sanJR_std$date), "%y-%m")
#sanJR_std$ymon1 <- as.Date(paste(sanJR_std$yrMon, "-01", sep=""))
sanJR_std$hour <- hour(sanJR_std$datetime)
sanJR_std$minute <- minute(sanJR_std$datetime)

# Remove unnecessaries
rm(delta_std_0, sacR_std_0, sanJR_std_0)
```

## Convert to hourly

1. Filter out NAs
2. Group by station, date, hour 
3. Slice (select) the first row for each group (above), delete the rest.

```{r hourly, message = FALSE}
# delta
delta_H_0 <- delta_std %>%
  filter(!is.na(Temp)) #remove all temperature NAs

delta_H <- delta_H_0 %>%
  group_by(station, date, hour) %>% #group (calculations) by these vars
  arrange(station, date, hour, minute) %>% #arrange in order of these vars to visualize duplication
  slice(1) %>% #subset only the first value for each station, date, hour group.
  ungroup()

# sacR
sacR_H_0 <- sacR_std %>%
  filter(!is.na(Temp)) 

sacR_H <- sacR_H_0 %>%
  group_by(station, date, hour) %>%
  arrange(station, date, hour, minute) %>%
  slice(1) %>%
  ungroup()

#sanJR
sanJR_H_0 <- sanJR_std %>%
  filter(!is.na(Temp))

sanJR_H <- sanJR_H_0 %>%
  group_by(station, date, hour) %>%
  arrange(station, date, hour, minute) %>%
  slice(1) %>%
  ungroup()

# Check that they are all good!
# Just change the name of the file. 
check <- delta_H %>%
  group_by(station, date, hour)%>%
  summarize(vals = n())

# this should be 0
which(check$vals >1)

```

## QA1) Remove days with less than 20 values 

1. Count the number of rows per station, date group. (There should only be one row per date)
2. To be deleted: Filter out dates for which there are less than 20 values (out of 24).
3. Use antijoin to remove the deleted dates from original data frame.

```{r 20 hours, message = FALSE}
  ### delta
# This data frame contains all the dates with less than 20 values. 
delta_q1_a <- delta_H %>%
  group_by(station, date) %>%
  arrange(station, date, hour) %>%
  summarize(total = n()) %>%
  ungroup()%>%
  filter(total<20)

#Join original dataframe with "delete" dataframe based on values NOT in common. 
#based on station and date
delta_q1 <- delta_H %>%
  anti_join(delta_q1_a, by = c("station", "date"))

# Check that the above is doing the right thing. 
a <- delta_q1 %>% filter(date == "2008-10-28")

  ### sacR
# This data frame contains all the dates with less than 20 values. 
sacR_q1_a <- sacR_H %>%
  group_by(station, date) %>%
  arrange(station, date, hour) %>%
  summarize(total = n()) %>%
  ungroup() %>%
  filter(total<20)

#Join original dataframe with "delete" dataframe based on values NOT in common. 
#based on station and date
sacR_q1 <- sacR_H %>%
  anti_join(sacR_q1_a, by = c("station", "date"))

# Check that the above is doing the right thing. 
a <- sacR_q1 %>% filter(date == "2014-01-15")

  ### sanJR
# This data frame contains all data with less than 20 date values. 
sanJR_q1_a <- sanJR_H %>%
  group_by(station, date) %>%
  arrange(station, date, hour) %>%
  summarize(total = n()) %>%
  filter(total<20)

#Join original dataframe with "delete" dataframe based on values NOT in common. 
#based on station and date
sanJR_q1 <- sanJR_H %>%
  anti_join(sanJR_q1_a)

# Check that the above is doing the right thing. 
a <- sanJR_q1 %>% filter(date == "2005-01-07")

# Remove unnecessaries
rm(delta_q1_a, sacR_q1_a, sanJR_q1_a)

```


## QA2) Remove if there are more than 9 repeating values in a row

1. Create new columns indicating whether the temperature at x hour is the same as that of x-1 hours, x-2 hours, x-3 hours, x-4 hours. (1 = yes, 0 = no)
2. Add the columns up. If all values are the same, "delete" = 4. 
3. Filter out rows to be deleted. 
4. Use antijoin to extract dates that are not in the "delete" data frame. 
    + Note: the "delete" data frame has only the rows where delete = 4, but we want to delete the entire day if there are     repeating values, so we need to specify by = station, date. 

```{r repeating values, message = FALSE}
library(dplyr)

### delta
delta_q1$Temp <- as.numeric(delta_q1$Temp)

# create columns that are temp-1, temp-2, temp-3, temp-4 (position-1, not actual temperature-1)
# for columns where there are 12 values repeating in a row (in one day), new variable "delete" = 1
delta_q2_a <- delta_q1 %>%
  group_by(station, date) %>%
  arrange(station, date, hour) %>%
  mutate(same1 = ifelse(Temp == lag(Temp, n= 1), 1, 0),
        same2 = ifelse(Temp == lag(Temp, n= 2), 1, 0),
        same3 = ifelse(Temp == lag(Temp, n= 3), 1, 0),
        same4 = ifelse(Temp == lag(Temp, n= 4), 1, 0),
        same5 = ifelse(Temp == lag(Temp, n=5), 1, 0),
        same6 = ifelse(Temp == lag(Temp, n= 6), 1, 0),
        same7 = ifelse(Temp == lag(Temp, n= 7), 1, 0),
        same8 = ifelse(Temp == lag(Temp, n= 8), 1, 0),
        same9 = ifelse(Temp == lag(Temp, n=9), 1, 0),
        same10 = ifelse(Temp == lag(Temp, n=10), 1, 0),
        same11 = ifelse(Temp == lag(Temp, n=11), 1, 0),
        delete = ifelse(same1 + same2 + same3 + same4 + same5 + same6 + 
                          same7 + same8 + same9 + same10 + same11 ==11, 1, 0)) %>%
    ungroup()

# create a dataframe of all the "delete" rows
delta_q2_b <- delta_q2_a %>%
  filter(delete == 1)

# Join original dataframe with "delete" dataframe based on values NOT in common. 
# based on station and date
# This does not only remove rows in "delete" table, but all data that match those dates.
delta_q2 <- delta_q1 %>%
  anti_join(delta_q2_b,by = c("station", "date"))

### sacR
sacR_q1$Temp <- as.numeric(sacR_q1$Temp)

# create columns that are temp-1, temp-2, temp-3, temp-4 (position-1, not actual temperature-1)
# for columns where there are 12 values repeating in a row (in one day), new variable "delete" = 1
sacR_q2_a <- sacR_q1 %>%
  group_by(station, date) %>%
  arrange(station, date, hour) %>%
  mutate(same1 = ifelse(Temp == lag(Temp, n= 1), 1, 0),
        same2 = ifelse(Temp == lag(Temp, n= 2), 1, 0),
        same3 = ifelse(Temp == lag(Temp, n= 3), 1, 0),
        same4 = ifelse(Temp == lag(Temp, n= 4), 1, 0),
        same5 = ifelse(Temp == lag(Temp, n=5), 1, 0),
        same6 = ifelse(Temp == lag(Temp, n= 6), 1, 0),
        same7 = ifelse(Temp == lag(Temp, n= 7), 1, 0),
        same8 = ifelse(Temp == lag(Temp, n= 8), 1, 0),
        same9 = ifelse(Temp == lag(Temp, n=9), 1, 0),
                same10 = ifelse(Temp == lag(Temp, n=10), 1, 0),
        same11 = ifelse(Temp == lag(Temp, n=11), 1, 0),
        delete = ifelse(same1 + same2 + same3 + same4 + same5 + same6 + 
                          same7 + same8 + same9 + same10 + same11 ==11, 1, 0)) %>%
  ungroup()

# create a dataframe of all the "delete" rows
sacR_q2_b <- sacR_q2_a %>%
  filter(delete == 1)

# Join original dataframe with "delete" dataframe based on values NOT in common. 
# based on station and date
# This does not only remove rows in "delete" table, but all data that match those dates.
sacR_q2 <- sacR_q1 %>%
  anti_join(sacR_q2_b,by = c("station", "date"))

### sanJR
sanJR_q1$Temp <- as.numeric(sanJR_q1$Temp)

# create columns that are temp-1, temp-2, temp-3, temp-4 (position-1, not actual temperature-1)
# for columns where there are 12 values repeating in a row (in one day), new variable "delete" = 1
sanJR_q2_a <- sanJR_q1 %>%
  group_by(station, date) %>%
  arrange(station, date, hour) %>%
  mutate(same1 = ifelse(Temp == lag(Temp, n= 1), 1, 0),
        same2 = ifelse(Temp == lag(Temp, n= 2), 1, 0),
        same3 = ifelse(Temp == lag(Temp, n= 3), 1, 0),
        same4 = ifelse(Temp == lag(Temp, n= 4), 1, 0),
        same5 = ifelse(Temp == lag(Temp, n=5), 1, 0),
        same6 = ifelse(Temp == lag(Temp, n= 6), 1, 0),
        same7 = ifelse(Temp == lag(Temp, n= 7), 1, 0),
        same8 = ifelse(Temp == lag(Temp, n= 8), 1, 0),
        same9 = ifelse(Temp == lag(Temp, n=9), 1, 0),
        same10 = ifelse(Temp == lag(Temp, n=10), 1, 0),
        same11 = ifelse(Temp == lag(Temp, n=11), 1, 0),
        delete = ifelse(same1 + same2 + same3 + same4 + same5 + same6 + 
                          same7 + same8 + same9 + same10 + same11 ==11, 1, 0)) %>%
  ungroup()

# create a dataframe of all the "delete" rows
sanJR_q2_b <- sanJR_q2_a %>%
  filter(delete == 1)

# Join original dataframe with "delete" dataframe based on values NOT in common. 
# based on station and date
# This does not only remove rows in "delete" table, but all data that match those dates.
sanJR_q2 <- sanJR_q1 %>%
  anti_join(sanJR_q2_b,by = c("station", "date"))

# Remove unnecessaries
rm(delta_q2_a, sacR_q2_a, sanJR_q2_a,
   delta_q2_b, sacR_q2_b, sanJR_q2_b)
```


## QA3): Remove anything outside of reasonable temperature range (0-40)
```{r 0-40, message = FALSE}
delta_q3 <- delta_q2%>%
  filter(Temp>0 & Temp<40)

sacR_q3 <- sacR_q2%>%
  filter(Temp>0 & Temp<40)

sanJR_q3 <- sanJR_q2%>%
  filter(Temp>0 & Temp<40)
```


## Q3: Merge and write files up to this point

* One large file with all stations: **allsta_q3**
* Individual files for each station: **STA_q3**

```{r q3 merge, message = FALSE, eval = FALSE}

allsta_q3 <- rbind(delta_q3, sacR_q3, sanJR_q3)

allsta_q3$qa <- "q3"
delta_q3$qa <- "q3"
sanJR_q3$qa <- "q3"
sacR_q3$qa <- "q3"

# Write large files
write_csv(delta_q3, "station_files/stations_qa/merged/delta_q3.csv")
write_csv(sacR_q3, "station_files/stations_qa/merged/sacR_q3.csv")
write_csv(sanJR_q3, "station_files/stations_qa/merged/sanJR_q3.csv")
write_csv(allsta_q3, "station_files/stations_qa/merged/allsta_q3.csv")

### Write individual files
# If loading file
# allsta_q3 <-  read_csv("station_files/stations_qa/merged/allsta_q3.csv")

allsta_q3$station <- as.factor(allsta_q3$station) # need to factorize the "stations"

#Get the list of unique MP names
for (name in levels(allsta_q3$station)) {
  #Subset the data station
  tmp=subset(allsta_q3,station==name)
  #Create a new filename for each station. Designate the folder you want the files in.
  fn=paste('station_files/stations_qa/preoutlier/',name, "_q3.csv", sep="")
  #Save the CSV file for each station
  write_csv(tmp,fn)
}
```

## Write q3 to global environment
```{r q3 global, message = FALSE, eval = FALSE}

allsta_q3 <- read_csv("station_files/stations_qa/merged/allsta_q3.csv")

### Write individual files to global environment
# If loading file
# allsta_q3 <-  read_csv("station_files/stations_qa/merged/allsta_q3.csv")

allsta_q3$station <- as.factor(allsta_q3$station) # need to factorize the "stations"

for (name in levels(allsta_q3$station)) {
  temp=subset(allsta_q3, station==name)
  fn=assign(paste0(name,"_q3"),temp, envir = globalenv())
  }
```

## QA4): Outlier method 1: Modified Z score method

1. Calculate the median, MAD (median absolute deviation) for each station, day combo.
2. Join back to original data
3. Calculate the modified Z score for each point (0.6745(xi - median)/MAD)
4. Filter out values with score > 3.5 (Iglewicz & Hoaglin 1993)

```{r MAD outlier, message = FALSE}
library(lubridate)
library(ggplot2)
### test
delta_q3$week <- week(delta_q3$date)
delta_q3$yrWk <- paste0(delta_q3$year, "-", delta_q3$week)
sacR_q3$week <- week(sacR_q3$date)
sacR_q3$yrWk <- paste0(sacR_q3$year, "-", sacR_q3$week)
sanJR_q3$week <- week(sanJR_q3$date)
sanJR_q3$yrWk <- paste0(sanJR_q3$year, "-", sanJR_q3$week)

### delta

# Calculate median, MAD, lower and upper ranges for each day
delta_q4_a <- delta_q3 %>%
  group_by(station, yrWk) %>%
   summarize(
    med = median(Temp),
    MAD = mad(Temp))
    
# Add calculated daily median, ranges to original table
delta_q4_b <- left_join(delta_q3, delta_q4_a, by = c("station", "yrWk"))

# Calculate modified Z-score
# 0.6745 = 75th percentile
# Delete temperatures not in range
delta_q4 <- delta_q4_b %>%
  group_by(station) %>%
  mutate(modZ = abs(0.6745*(Temp - med)/MAD)) %>%
  filter(modZ < 3.5)
  
### sacR
# Calculate median, MAD, lower and upper ranges for each day
sacR_q4_a <- sacR_q3 %>%
  group_by(station, yrWk) %>%
  summarize(
    med = median(Temp),
    MAD = mad(Temp))

# Add calculated daily median, ranges to original table
sacR_q4_b <- left_join(sacR_q3, sacR_q4_a, by = c("station", "yrWk"))

# Calculate modified Z-score
# 0.6745 = 75th percentile
sacR_q4 <- sacR_q4_b %>%
  group_by(station) %>%
  mutate(modZ = abs(0.6745*(Temp - med)/MAD)) %>%
  filter(modZ < 3.5)

### sanJR
# Calculate median, MAD, lower and upper ranges for each day
sanJR_q4_a <- sanJR_q3 %>%
  group_by(station, yrWk) %>%
  summarize(
    med = median(Temp),
    MAD = mad(Temp))

# Add calculated daily median, ranges to original table
sanJR_q4_b <- left_join(sanJR_q3, sanJR_q4_a, by = c("station", "yrWk"))

# Calculate modified Z-score
# 0.6745 = 75th percentile
sanJR_q4 <- sanJR_q4_b %>%
  group_by(station) %>%
  mutate(modZ = abs(0.6745*(Temp - med)/MAD)) %>%
  filter(modZ < 3.5)

# Remove unnecessaries
rm(delta_q4_a, sacR_q4_a, sanJR_q4_a)
```

## QA5) Outlier Method 2: Tukey Method
1. Calculate Q1, Q3, IQR for each station, date grouping
2. Join back to original data
3. Filter out values below and above Q1-1.5IQR, Q3+1.5IQR

```{r MAD outlier, message = FALSE}
### delta
 
# Calculate median, MAD, lower and upper ranges for each day
delta_q5_a <- delta_q3 %>%
  group_by(station, yrWk) %>%
   summarize(Q1 = quantile(Temp, probs = 0.25), 
            Q3 = quantile(Temp, probs = 0.75),
            IQR = Q3-Q1,
            ul = Q3 + 1.5 * IQR,
            ll = Q1 - 1.5 * IQR)
    
# Add calculated daily median, ranges to original table
delta_q5_b <- left_join(delta_q3, delta_q5_a, by = c("station", "yrWk"))

# Delete temperatures outside of range
delta_q5 <- delta_q5_b %>%
  filter(Temp>ll & Temp<ul)

### sacR
# Calculate median, MAD, lower and upper ranges for each day
sacR_q5_a <- sacR_q3 %>%
  group_by(station, yrWk) %>%
  summarize(Q1 = quantile(Temp, probs = 0.25), 
            Q3 = quantile(Temp, probs = 0.75),
            IQR = Q3-Q1,
            ul = Q3 + 1.5 * IQR,
            ll = Q1 - 1.5 * IQR)

# Add calculated daily median, ranges to original table
sacR_q5_b <- left_join(sacR_q3, sacR_q5_a, by = c("station", "yrWk"))

# Delete temperatures outside of lower or upper ranges
sacR_q5 <- sacR_q5_b %>%
  filter(Temp>ll & Temp<ul)

### sanJR
# Calculate median, MAD, lower and upper ranges for each day
sanJR_q5_a <- sanJR_q3 %>%
  group_by(station, yrWk) %>%
   summarize(Q1 = quantile(Temp, probs = 0.25), 
            Q3 = quantile(Temp, probs = 0.75),
            IQR = Q3-Q1,
            ul = Q3 + 1.5 * IQR,
            ll = Q1 - 1.5 * IQR)

# Add calculated daily median, ranges to original table
sanJR_q5_b <- left_join(sanJR_q3, sanJR_q5_a, by = c("station", "yrWk"))

# Delete temperatures outside of lower or upper ranges
sanJR_q5 <- sanJR_q5_b %>%
  filter(Temp>ll & Temp<ul)

# Remove unnecessaries
rm(delta_q5_a, sacR_q5_a, sanJR_q5_a)
```


## Q4: Merge the finished products

* One large file with all stations: **allsta_q4**
* Individual files for each station: **STA_q4**

```{r q4 merge, message = FALSE}
library(readr)
### One large file
allsta_q4 <- rbind(delta_q4, sacR_q4, sanJR_q4)
allsta_q4$qa <- "q4"
delta_q4$qa <- "q4"
sacR_q4$qa <- "q4"
sanJR_q4$qa <- "q4"
write_csv(delta_q4, "station_files/stations_qa/merged/delta_q4.csv")
write_csv(sacR_q4, "station_files/stations_qa/merged/sacR_q4.csv")
write_csv(sanJR_q4, "station_files/stations_qa/merged/sanJR_q4.csv")
write_csv(allsta_q4, "station_files/stations_qa/merged/allsta_q4.csv")

### Individual files to csv
# If loading file
allsta_q4 <-  read_csv("station_files/stations_qa/merged/allsta_q4.csv")

# write each file as a csv
allsta_q4$station <- as.factor(allsta_q4$station) # need to factorize the "stations"
#Get the list of unique MP names
for (name in levels(allsta_q4$station)) {
  #Subset the data station
  tmp=subset(allsta_q4,station==name)
  #Create a new filename for each station. Designate the folder you want the files in.
  fn=paste('station_files/stations_qa/postoutlier/',name, "_q4.csv", sep="")
  #Save the CSV file for each station
  write_csv(tmp,fn)
}
```


## Q5: Merge the finished products

* One large file with all stations: **allsta_q4**
* Individual files for each station: **STA_q4**

```{r q5 merge, message = FALSE}
library(readr)
### One large file
allsta_q5 <- rbind(delta_q5, sacR_q5, sanJR_q5)
allsta_q5$qa <- "q5"
delta_q5$qa <- "q5"
sacR_q5$qa <- "q5"
sanJR_q5$qa <- "q5"
write_csv(delta_q5, "station_files/stations_qa/merged/delta_q5.csv")
write_csv(sacR_q5, "station_files/stations_qa/merged/sacR_q5.csv")
write_csv(sanJR_q5, "station_files/stations_qa/merged/sanJR_q5.csv")
write_csv(allsta_q5, "station_files/stations_qa/merged/allsta_q5.csv")

### Individual files to csv
# If loading file
#allsta_q5 <-  read_csv("station_files/stations_qa/merged/allsta_q5.csv")

# write each file as a csv
allsta_q5$station <- as.factor(allsta_q5$station) # need to factorize the "stations"
#Get the list of unique MP names
for (name in levels(allsta_q5$station)) {
  #Subset the data station
  tmp=subset(allsta_q5,station==name)
  #Create a new filename for each station. Designate the folder you want the files in.
  fn=paste('station_files/stations_qa/postoutlier/',name, "_q5.csv", sep="")
  #Save the CSV file for each station
  write_csv(tmp,fn)
}

```


## Write q4 station files to global environment

```{r q4 global, message = FALSE}
# Read in overall files (if needed)
setwd("P:/ClimateChange/R_code/")
#allsta_q4 <- read_csv("station_files/stations_qa/merged/allsta_q4.csv")

### Write each station file to global environment
allsta_q4$station <- as.factor(allsta_q4$station) # need to factorize the "stations"

for (name in levels(allsta_q4$station)) {
  temp=subset(allsta_q4, station==name)
  fn=assign(paste0(name,"_q4"),temp, envir = globalenv())
  }

# In case you want to read one station in: 
#BAC_q4 <- allsta_q4 %>% filter(station == "BAC")


```

## Write q5 station files to global environment

```{r q5 global, message = FALSE}
# Read in overall files (if needed)
setwd("P:/ClimateChange/R_code/")
#allsta_q5 <- read_csv("station_files/stations_qa/merged/allsta_q5.csv")

### Write each station file to global environment
allsta_q5$station <- as.factor(allsta_q5$station) # need to factorize the "stations"

for (name in levels(allsta_q5$station)) {
  temp=subset(allsta_q5, station==name)
  fn=assign(paste0(name,"_q5"),temp, envir = globalenv())
  }

# In case you want to read one station in: 
#BAC_q5 <- allsta_q5 %>% filter(station == "BAC")


```

