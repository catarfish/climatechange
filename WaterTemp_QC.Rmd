---
title: "WaterTemp_QC"
author: "Catarina Pien"
date: "May 21, 2020"
output: html_document
description: QC Steps for CDEC data
editor_options: 
chunk_output_type: console
---

---------------------------------------------------------------------------------
In this file, we take the raw hourly CDEC data compiled from DownloadData_CDEC.Rmd. We then run the data through numerous QC tests that were informed by NOAA's Manual for Real-Time Quality Control of In-situ Temperature and Salinity Data:
https://cdn.ioos.noaa.gov/media/2017/12/qartod_temperature_salinity_manual.pdf

Two files are written, one that has suspect data flagged, one that has all suspect data filtered out. You can change the settings of the QC filters to fit your needs, and use the accompanying RShiny app to visualize how changing filters affects what data gets flagged. 
------------------------------------------------------------------------------------


Start by clearing the environment and loading packages.

```{r setup, include = FALSE}
rm(list=ls(all=TRUE))

library(tidyverse)
library(readr)
library(lubridate)
library(TTR) # rate of change
library(caTools) # rate of change
```

## Load files, edit variable names

* Filter out those that are not contiguous/ not active
* Add datetime sorting variables
```{r data, message = FALSE, warning = FALSE}
#setwd("C:/Users/cpien/OneDrive - California Department of Water Resources/Work/ClimateChange/R_code/")
#### Read files ###
temp_H_0 <- readRDS("WaterTemp/data/Temp_all_H.rds")
latlons <- read.csv("WaterTemp/data/StationsMetadata.csv")

# Optional for filtering out non-contiguous sites
temp_H <- temp_H_0
#filter(!Station %in% c("CNT", "CPP", "DAR", "DMC", "DYR","ECD", "HBP", "ROR", "DV7")) 

# Filter out rows that have "NA" for date/time
temp_H <- temp_H %>%
  filter(!is.na(temp_H$Datetime))

# Do some renaming
# Optional for filtering out non-contiguous sites
# filter(!station %in% c("CNT", "CPP", "DAR", "DMC", "DYR", "ECD", "HBP",  "ROR", "DV7")) %>%
latlonmin <- as.data.frame(select(latlons, 1:2)) # For merging at the end

# Look at data
str(temp_H)
head(temp_H)

# Remove unnecessaries to keep from exceeding memory limit (optional)
rm(temp_H_0)

```

## QC1) Flag data outside of reasonable temperature range (1-40C)
```{r 0-40, message = FALSE}

# Data including flags
temp_q1 <- temp_H %>% mutate(Flag_QC1 = ifelse(Temp<1 | Temp>40, "Y", "N")) 

# Flagged values only
temp_q1_b <- temp_q1 %>%
  filter(Flag_QC1 == "Y")

```

## QC2) Missing values: Flag days with less than n(20) values 

1. Count the number of rows per station, date group. (There should only be one row per date)
2. Flag days where there are less than 20 values (out of 24 - hourly data).
3. Use leftjoin to add flags to the original data.

```{r Missing Values, message = FALSE}

# This data frame contains all the dates with less than 20 values. 
temp_q2_a <- temp_q1 %>%
  filter(Flag_QC1 == "N") %>% # only running on data that have not been flagged by the above to be consistent with rest of QC steps
  group_by(Station, Date) %>%
  arrange(Station, Date, Hour) %>%
  summarise(total = n()) %>%
  mutate(Flag_QC2 = ifelse(total < 20, "Y", "N")) %>%
  select(-total) %>%
  ungroup()

# Flagged values
temp_q2_b <- temp_q2_a %>%
  filter(Flag_QC2 == "Y")

#Join original dataframe with flagged values based on values NOT in common. 
#based on station and date
temp_q2 <- temp_q1 %>%
   left_join(temp_q2_a, by = c("Station", "Date")) %>%
   filter(Flag_QC1 == "N") # This part is important for QC5 and QC6. Basically removes all values that are not within range (QC1) 
    # for BET (maybe other stations) there were some alternately repeating values near 0 that were causing lots of spike QCs to be positive.
   #filter(Flag_QC1 == "N") 
```

## QC3) Flag if there are 18+ repeating values in a row

1. Create new columns indicating whether the temperature at x hour is the same as that of x-1 hours.
2. Take a cumulative sum of all the rows where temperatures are different
3. Group by the sum and count up the number of rows where the temperature is the same.
4. Flag the rows where number of repeated values is above our cut-off

```{r repeating values, message = FALSE}

#########################################################
# Significant help from Michael Koohafkan and Rosie Hartman

# Function to determine whether values are repeating by each station
# Inputs are data frame and x (number of repeating values you want to check for)
# Check if number is same as previous number. If yes, 1. If no, 0.
# Cumulative sum so each time a value repeats, cumulative sum goes up
# Count the number in a row that are the same
# Flag if that number > threshold 

repeating_vals = function(df, x){
  df$same = ifelse(df$Temp == lag(df$Temp, 1, default = 0), 1L, 0L)
  df = df %>%
    mutate(issame = cumsum(df$same == 0L)) %>%
    group_by(Station, issame) %>%
    mutate(flag = sum(same)+1 ) %>%
    ungroup() %>%
    mutate(Flag_repeats = ifelse(flag > x, "Y", "N"))
  return(df)
}
###########################################################

# Run function repeating values and get rid of the columns we don't need
temp_q3 <- repeating_vals(df = temp_q2, x = 18) %>%
  select(-flag, -issame, -same) %>%
  rename(Flag_QC3 = Flag_repeats) 

# Flagged values
temp_q3_b <- temp_q3 %>%
  filter(Flag_QC3 == "Y")

# Remove earlier files
rm(temp_q2_a)

```


## QC4) Use the anomalize package to flag anomalies
* This is slow!!
* Twitter + GESD is for more for highly seasonal data (however, GESD is extremely slow because it is iterative)
* STL + IQR if seasonality is not a major factor
* Trend period depends on personal knowledge of data, we analyzed a subset of data to see what worked best

```{r anomalize}
library(anomalize)
library(tibbletime)
# see https://business-science.github.io/anomalize/articles/anomalize_methods.html
  
# Convert data frame to table 
temp_q4_a <- as_tbl_time(temp_q3, index = Datetime)

# Anomaly Detection
# time_decompose: separates time series into seasonal, trend, and remainder components
  # stl: loess works well when long term trend is present
  # twitter: (removes median rather than fitting smoother) - when long-term trend is less dominant than short-term seasonal component
# anomalize: applies anomaly detection methods to remainder component
# time_recompose: calculate limits to separate "normal" data from anomalies
temp_q4_c <- temp_q4_a %>%
    group_by(Station) %>%
    time_decompose(Temp, method = "stl", trend = "6 months") %>%
    anomalize(remainder, method = "iqr", alpha = 0.05) %>%
    time_recompose() %>% 
    select(c(Datetime, anomaly)) %>%
    as_tibble() 

# Join "anomaly" with rest of the data
temp_q4_d <- inner_join(temp_q3, temp_q4_c, by = c( "Datetime", "Station"))

# Rename "anomaly" Flag_QC4 for consistency, change No to N and Yes to Y
temp_q4 <- temp_q4_d %>%
    mutate(anomaly = factor(anomaly)) %>%
    mutate(anomaly = recode(anomaly, No = "N", Yes = "Y"))  %>%
    rename(Flag_QC4 = anomaly)
    
# Flagged values
temp_q4_b <- temp_q4 %>%
    filter(Flag_QC4 == "Y")

```

### QC5) Spike test
- Modified from https://github.com/SuisunMarshBranch/wqptools/blob/master/R/rtqc.r

Anomalize is pretty good, but there are a few single points here and there that don't get detected. 
1. If |temp(t) - mean(temp(t-1) + temp(t+1))| > 5, it is flagged.

```{r Spike test}
### ---------------------------------------------
# Q5: Temp - Temp@time-1
# Additionally, if Q5 > 5 (5 degree change in 1 hour), flag. 

temp_q5 <- temp_q4 %>%
  group_by(Station) %>%
  arrange(Station, Datetime) %>%
  mutate(QC5 = abs(Temp- 0.5 * (lag(Temp, n = 1, default = 0) + lead(Temp, n=1, default = 0))))%>%
  mutate(Flag_QC5 = ifelse((QC5 > 5), "Y", "N"))  %>%
      mutate(Flag_QC5 = replace(Flag_QC5, is.na(Flag_QC5), "N")) %>% # Replace NA with No
  select(-QC5) %>%
  ungroup()

# Flagged values
temp_q5_b <- temp_q5 %>%
  filter(Flag_QC5 == "Y")

# Remove data 
rm(temp_q4_a, temp_q4_c, temp_q4_d)

```


### QC6) Rate of Change Test
1. Group by Station, Datetime
2. Define standard deviation threshold (e.g. 5 * sd(last 50 hours)) - could change it to be greater or lesser
3. If difference between value and the value before it > threshold, it is flagged.

```{r Rate of Change}
# Q6 = Temp - Temp@time-1
# sdev_th: Determined threshold for too high of a rate of change (5 * SD(Temp) over 50 hours or 2 tidal cycles)
# If Q6 > sdev_th, flag.

temp_q6 <- temp_q5 %>%
  group_by(Station) %>%
  arrange(Station, Datetime) %>%
  mutate(QC6 = abs(Temp- lag(Temp, n = 1, default = 0)))%>%
  mutate(sdev_th = 5 * runSD(Temp, 50))%>%
  mutate(Flag_QC6 = ifelse((QC6 > sdev_th), "Y", "N"))  %>%
      mutate(Flag_QC6 = replace(Flag_QC6, is.na(Flag_QC6), "N")) %>% # Replace NA with No
  select(-c(QC6, sdev_th)) %>%
  ungroup()

# Flagged values
temp_q6_b <- temp_q6 %>%
  filter(Flag_QC6 == "Y")

```

## Filter final 
1. Add back in flagged data from QC1
2. Combine flags in one column
3. Create flagged dataset and filtered dataset

```{r final dataset}
# Merge back in q1 data since these were removed
# Data that were filtered out were not subsequently run under other QC tests, so NA
temp_q1_table <- temp_q1 %>%
  filter(Flag_QC1 == "Y") %>%
      mutate(Flag_QC2 = "NA",
             Flag_QC3 = "NA", 
             Flag_QC4 = "NA",
             Flag_QC5 = "NA",
             Flag_QC6 = "NA")

# Combine Flags from QC1 with rest of flags
temp_flags <- rbind(temp_q6, temp_q1_table) %>%
  ungroup() %>%
  mutate(AllFlags = paste0(Flag_QC1, ",", Flag_QC2, ",", Flag_QC3, ",", Flag_QC4, ",", Flag_QC5, ",", Flag_QC6)) %>%
        left_join(latlonmin, by = "Station") %>%
  select(-Hour) %>%
      select(Station, StationName, Datetime, Date, everything())


# Filtered dataset - only rows that do not have any flags
temp_final <- temp_flags %>%
  filter(grepl("N,N,N,N,N,N", AllFlags)) %>%
  select(-c(contains("Flag"))) %>%
        select(Station, StationName, Datetime, Date, everything())

```

## Write individual and combined station files (Optional)
* For Flagged and Filtered datasets:
- RDS, CSV, individual csv for each Station
- RDS files are faster to read and write, but you need to use R to read them (can't just open in Excel like a CSV)

```{r WriteFile, message = FALSE}

# Combined files - flagged
saveRDS(temp_flags, "TempData/QC/Temp_flagged.rds")
write_csv(temp_flags, "TempData/QC/Temp_flagged.csv")

# Combined files - filtered
saveRDS(temp_final, "TempData/QC/Temp_filtered.rds")
write_csv(temp_final, "TempData/QC/Temp_filtered.csv")

#----------------------------------------------------------------------------------
## ONLY RUN THIS NEXT SECTION IF YOU WANT INDIVIDUAL STATION FILES! IT WILL BE SLOW. ##
#----------------------------------------------------------------------------------

### Individual files to csv (flagged data)
# write each file as a csv
temp_flags$Station <- as.factor(temp_flags$Station) # need to factorize the "Stations"
#Get the list of unique Station names
for (name in levels(temp_flags$Station)) {
  #Subset the data Station
  tmp=subset(temp_flags,Station==name)
  #Create a new filename for each Station. Designate the folder you want the files in.
  fn=paste('TempData/QC/Individual/Flagged/',name, "_flagged.csv", sep="")
  #Save the CSV file for each Station
  write_csv(tmp,fn)
}

### Individual files to csv (filtered data)
# write each file as a csv
temp_final$Station <- as.factor(temp_final$Station) # need to factorize the "Stations"
#Get the list of unique Station names
for (name in levels(temp_final$Station)) {
  #Subset the data Station
  tmp=subset(temp_final,Station==name)
  #Create a new filename for each Station. Designate the folder you want the files in.
  fn=paste('TempData/QC/Individual/Filtered/',name, "_qc.csv", sep="")
  #Save the CSV file for each Station
  write_csv(tmp,fn)
}
```


## How much data are being removed?

```{r Data removal}
# By station 
(Flagged_stations <- temp_flags %>%
  group_by(Station) %>%
  summarize(Init = n(),
            QC1 = sum(Flag_QC1=="Y"),
            QC2 = sum(Flag_QC2 == "Y"),
            QC3 = sum(Flag_QC3 == "Y"),
            QC4 = sum(Flag_QC4 == "Y"),
            QC5 = sum(Flag_QC5 == "Y"),
            QC6 = sum(Flag_QC6 == "Y"),
            QCTot = sum(grepl("Y", AllFlags)),
            Pct_Flagged_QC1 = round(QC1/Init*100,2),
            Pct_Flagged_QC2 = round(QC2/Init*100,2),
            Pct_Flagged_QC3 = round(QC3/Init*100,2),
            Pct_Flagged_QC4 = round(QC4/Init*100,2),
            Pct_Flagged_QC5 = round(QC5/Init*100,2),
            Pct_Flagged_QC6 = round(QC6/Init*100,2),
            Pct_Flagged_Total = round(QCTot/Init * 100,2) ))

write_csv(Flagged_stations, "Tempdata/QC/Flagged_Percentages.csv")

# Overall dataset
Flags <- temp_flags %>%
  filter(grepl("Y", AllFlags))
print(paste0(round(nrow(Flags)/nrow(temp_q6)*100,2), "% flagged"))

```

